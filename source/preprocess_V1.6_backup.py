import sys
import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from feature_engine import imputation as mdi
from feature_engine import encoding as ce
from feature_engine.discretisation import EqualFrequencyDiscretiser
from feature_engine.discretisation import EqualWidthDiscretiser
from feature_engine.encoding import OneHotEncoder
from dateutil.parser import parse
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
#from feature_engine.imputation import MeanMedianImputer
#from feature_engine.encoding import OneHotEncoder, OrdinalEncoder
#from feature_engine.discretisation import EqualFrequencyDiscretiser, EqualWidthDiscretiser
from sklearn.base import BaseEstimator, TransformerMixin # CustomOrdinalEncoderë¥¼ ìœ„í•´ ì¶”ê°€
from typing import List, Union # CustomOrdinalEncoderë¥¼ ìœ„í•´ ì¶”ê°€

import mean_median2 as mm
import temporal_feature as tf
from pathlib import Path
import warnings
import traceback
import json

warnings.filterwarnings('ignore')


def join_abs_path(p1, p2):
    return os.path.abspath(os.path.join(p1, p2))
class CustomOrdinalEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, variables: Union[None, List[str]] = None,
                 encoding_method: str = 'ordered',
                 suffix: str = '_ordinal'):

        if variables is not None and not isinstance(variables, list):
            self.variables = [variables]
        else:
            self.variables = variables

        self.encoding_method = encoding_method
        self.suffix = suffix
        self.encoder_ = None # feature-engineì˜ OrdinalEncoder ì¸ìŠ¤í„´ìŠ¤
        self.feature_names_in_ = None # sklearn ìŠ¤íƒ€ì¼ ì…ë ¥ íŠ¹ì„± ì´ë¦„

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        if self.variables is None:
            raise ValueError("Parameter 'variables' must be provided as a list of column names.")

        self.variables_ = self.variables # ì‹¤ì œë¡œ ë³€í™˜ë  ë³€ìˆ˜ë“¤
        self.feature_names_in_ = X.columns.tolist()

        self.encoder_ = ce.OrdinalEncoder(
            encoding_method=self.encoding_method,
            variables=self.variables_
        )
        self.encoder_.fit(X, y)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X_input_for_internal_encoder = X.copy()

        # encoder_ê°€ variables_ì— ì§€ì •ëœ ì»¬ëŸ¼ë“¤ë§Œ ë³€í™˜í•œ DataFrameì„ ë°˜í™˜
        X_processed_by_internal_encoder = self.encoder_.transform(X_input_for_internal_encoder)
        X_output = X.copy()

        for var in self.variables_:
            new_col_name = f"{var}{self.suffix}"
            X_output[new_col_name] = X_processed_by_internal_encoder[var] # ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€ (DataFrame ëì— ë¶™ìŒ)

        X_output = X_output.drop(columns=self.variables_)
        return X_output

    def get_feature_names_out(self, input_features: List[str] = None) -> List[str]:
        if input_features is None:
            input_features = self.feature_names_in_
        output_features = [col for col in input_features if col not in self.variables_]
        for var in self.variables_:
            output_features.append(f"{var}{self.suffix}")
        return output_features

def position_Y_COL(cols):  # Y labelì„ ê°€ì¥ ë’¤ë¡œ ìœ„ì¹˜ ë³€ê²½
    if Y_COL in cols:  # Y_COLì´ ìˆì„ ë•Œë§Œ remove ì‹¤í–‰
        cols_copy = cols.copy()
        cols_copy.remove(Y_COL)
        return cols_copy + [Y_COL]
    else:  # Y_COLì´ ì—†ìœ¼ë©´ ë³€ê²½ì—†ì´ ë¦¬í„´
        return cols

# ì•ˆì „í•œ ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜
def safe_parse(date_string):
    try:
        return parse(date_string)
    except (ValueError, TypeError):
        return None  # ë˜ëŠ” ë‹¤ë¥¸ ì˜¤ë¥˜ ì²˜ë¦¬ ë°©ë²•

def read_data(afile):    
    #ë‚ ì§œì»¬ëŸ¼ì€ temporal_feature.pyì—ì„œ ì²˜ë¦¬í•¨
    # # Case 1: ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°
    # if config_dict['date_col'] is np.nan:
    #     df = pd.read_csv(afile, usecols=config_dict['keep_col'])
    # # Case 2: ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
    # else:
    #     #date_colì´ ì—¬ëŸ¬ê°œë©´ ì•ˆëŒì•„ê°ˆ ìˆ˜ ìˆìŒ
    #     # # Case 2-1: date_colì´ keep_colì— í¬í•¨ëœ ê²½ìš°
    #     # if config_dict['date_col'] in config_dict['keep_col']: 

    #     # Case 2-1: date_col ì¤‘ í•˜ë‚˜ë¼ë„ keep_colì— í¬í•¨ëœ ê²½ìš°
    #     if any(col in config_dict['keep_col'] for col in config_dict['date_col']):
    #         df = pd.read_csv(afile, usecols=config_dict['keep_col'], parse_dates=config_dict['date_col'])
    #         # date_colì´ ì •ë§ dateí˜•ì¸ì§€ í™•ì¸
    #         for col in config_dict['date_col']:
    #             if not pd.api.types.is_datetime64_any_dtype(df[col]):
    #                 df[col] = df[col].apply(safe_parse)

    #     # Case 2-2: date_colì´ keep_colì— ì—†ëŠ” ê²½ìš°
    #     else:
    #         df = pd.read_csv(afile, usecols=config_dict['keep_col'])
    
    df = pd.read_csv(afile, usecols=config_dict['keep_col'],encoding='utf-8')
    cols = list(df.columns)
    cols = position_Y_COL(cols)
    return df[cols]  


def y_label_enc(df):
    df = df.copy()
    # íƒ€ê²Ÿ ë³€ìˆ˜(Y_COL)ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸
    if df[Y_COL].isnull().any():
        Y_null_exist = True
    else:
        Y_null_exist = False
    labeler = LabelEncoder()
    # íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”©
    df[Y_COL] = labeler.fit_transform(df[Y_COL])
    return df, Y_null_exist

#ë¶„ë¥˜ë³„ ì»¬ëŸ¼ ë¶„ë¥˜(discrete:ì…€ìˆ˜ìˆìŒ, continuous:ì—°ì†í˜•, categorical:ì˜¤ë¸Œì íŠ¸, ê·¸ì™¸ ë‚ ì§œ ë°ì´í„°)
def discrete_cont(df):
    # ì›ë³¸ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ ì¹´í”¼í•˜ì—¬ ì‘ì—…í•¨
    data = df.copy()
    # ë‚ ì§œí˜•, ì‹œê°„í˜•
    date_cols_len = len(config_dict['date_col']) if config_dict['date_col'] and not pd.isna(config_dict['date_col'][0]) else 0
    
    # jsoní˜•
    dict_cols_len = len(config_dict['dict_col']) if config_dict['dict_col'] and not pd.isna(config_dict['dict_col'][0]) else 0
    # ë²¡í„°í˜•
    vector_cols_len = len(config_dict['vector_col']) if config_dict['vector_col'] and not pd.isna(config_dict['vector_col'][0]) else 0
    # ì§„ë²•í˜•
    non_dec_cols_len = len(config_dict['non_dec_col']) if config_dict['non_dec_col'] and not pd.isna(config_dict['non_dec_col'][0]) else 0
    # ë¬¸ì¥í˜•
    sentence_cols_len = len(config_dict['sentence_col']) if config_dict['sentence_col'] and not pd.isna(config_dict['sentence_col'][0]) else 0

    # Case 1 : ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´
    if date_cols_len < 1:
        # ì´ì‚°í˜• ë³€ìˆ˜: ìˆ«ìí˜•ì´ë©´ì„œ ê³ ìœ ê°’ì´ ì„ê³„ê°’ë³´ë‹¤ ì ì€ ê²½ìš°
        discrete = [var for var in data.columns if
                    data[var].dtype != 'O' and var != Y_COL and data[var].nunique() < config_dict['discrete_thresh_hold']]
        # ì—°ì†í˜• ë³€ìˆ˜: ìˆ«ìí˜•ì´ë©´ì„œ ì´ì‚°í˜•ì´ ì•„ë‹Œ ê²½ìš°
        continuous = [var for var in data.columns if
                      data[var].dtype != 'O' and var != Y_COL and var not in discrete]
    # Case 2 : ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´
    else:
        # ì´ì‚°í˜• ë³€ìˆ˜: ìˆ«ìí˜•ì´ë©´ì„œ ê³ ìœ ê°’ì´ ì„ê³„ê°’ë³´ë‹¤ ì ì€ ê²½ìš° ë° ë‚ ìì»¬ëŸ¼ì´ ì•„ë‹Œ ê²½ìš°
        discrete = [var for var in data.columns if
                    data[var].dtype != 'O' and var != Y_COL and var not in config_dict['date_col']
                    and var not in config_dict['dict_col'] and var not in config_dict['vector_col']
                    and var not in config_dict['non_dec_col'] and var not in config_dict['sentence_col']
                    and data[var].nunique() < config_dict['discrete_thresh_hold']]
        # ì—°ì†í˜• ë³€ìˆ˜: ìˆ«ìí˜•ì´ë©´ì„œ ì´ì‚°í˜•ì´ ì•„ë‹Œ ê²½ìš° ë° ë‚ ìì»¬ëŸ¼ì´ ì•„ë‹Œ ê²½ìš°
        continuous = [var for var in data.columns if
                      data[var].dtype != 'O' and var != Y_COL and var not in config_dict['date_col']
                      and var not in config_dict['dict_col'] and var not in config_dict['vector_col']
                      and var not in config_dict['non_dec_col'] and var not in config_dict['sentence_col']
                      and var not in discrete]

    # categorical
    # ê°ì²´í˜•(ë¬¸ìì—´) ë°ì´í„°ì´ë©´ì„œ íƒ€ê²Ÿë³€ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš°
    categorical = [var for var in data.columns if
                   data[var].dtype == 'O' and var != Y_COL and var not in config_dict['date_col']
                   and var not in config_dict['dict_col'] and var not in config_dict['vector_col']
                   and var not in config_dict['non_dec_col'] and var not in config_dict['sentence_col']]
    
    # ì „ì²˜ë¦¬ ë°ì´í„° íƒ€ì… í™•ì¸ìš©
    print(f'There are {date_cols_len} date_time variables')
    print(f'There are {dict_cols_len} dict variables')
    print(f'There are {vector_cols_len} vector variables')
    print(f'There are {non_dec_cols_len} non-decimal variables')
    print(f'There are {sentence_cols_len} sentence variables')
    print(f'There are {len(discrete)} discrete variables')
    print(f'There are {len(continuous)} continuous variables')
    print(f'There are {len(categorical)} categorical variables')
    return discrete, continuous, categorical


def separate_mixed(df):
    df = df.copy()    
    s = config_dict['mixed_str'][0]
    e = config_dict['mixed_str'][1]
    mixed_col = config_dict['mixed'][0]
    insert_at = df.columns.get_loc(mixed_col)
    num_col = df[mixed_col].str.extract(r'(\d+)').astype('float')
    num_col.columns = [mixed_col + 'num']
    cat_col = df[mixed_col].str[s:e]
    cat_col.name = mixed_col + 'cat'
    df.drop([mixed_col], axis=1, inplace=True)
    left = df.iloc[:, :insert_at]
    right = df.iloc[:, insert_at:]
    df = pd.concat([left, num_col, cat_col.to_frame(), right], axis=1)
    cols = position_Y_COL(list(df.columns))
    return df[cols]

# ì†Œìˆ˜í˜•ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ
def truncate_to_integer(series):
    # ëª¨ë“  ê°’ì´ 1ë³´ë‹¤ í´ ë•Œê¹Œì§€ 10ì„ ê³±í•¨
    while (series < 1).any():
        series *= 10
    
    # ì†Œìˆ˜ì  ì´í•˜ ì˜ë¼ë‚´ê³  ì •ìˆ˜ë¡œ ë³€í™˜
    truncated_series = series.astype(int)
    
    # ì›ë³¸ ê°’ê³¼ ë³€í™˜ëœ ê°’ì˜ ê´€ê³„ ì €ì¥(XAI í•„ìš”í•˜ë©´ ì‚¬ìš©)
    #value_map = pd.Series(truncated_series.values, index=series.values)
    return truncated_series

# ì •ìˆ˜í˜•ì˜ 1ì˜ ìë¦¬ë¥¼ ë²„ë¦¼
def truncate_to_ten(series):
    series /= 10
    # ì†Œìˆ˜ì  ì´í•˜ ì˜ë¼ë‚´ê³  ì •ìˆ˜ë¡œ ë³€í™˜
    truncated_series = series.astype(int)
    truncated_series *= 10
    # ì›ë³¸ ê°’ê³¼ ë³€í™˜ëœ ê°’ì˜ ê´€ê³„ ì €ì¥(XAI í•„ìš”í•˜ë©´ ì‚¬ìš©)
    #value_map = pd.Series(truncated_series.values, index=series.values)
    return truncated_series

def discretiser(df, numeric):
    df = df.copy()
    method = config_dict['discretiser_type'][0]
    cols = config_dict['discretiser']
    for col in cols:  # ê° ì—´ì— ëŒ€í•´ ë°˜ë³µ
        if method == 'equalwidth':
            trans = EqualWidthDiscretiser()
            X = df[[col]]
            trans.fit(X)
            df[col] = trans.transform(X)[col]
        elif method == 'equalfrequency':
            trans = EqualFrequencyDiscretiser()
            X = df[[col]]
            trans.fit(X)
            df[col] = trans.transform(X)[col]
        elif method == 'equalfixed':
            
            # ì‹¤ìˆ˜í˜•ì´ë©´ truncate_to_integer í•¨ìˆ˜ í˜¸ì¶œ
            if np.issubdtype(df[col].dtype, np.floating):  # ì‹¤ìˆ˜í˜• í™•ì¸
                truncated_data = truncate_to_integer(df[col])
                df[col] = truncated_data  # ë³€í™˜ëœ ì •ìˆ˜í˜• ë°ì´í„°ë¡œ ëŒ€ì²´
            # ì •ìˆ˜í˜•ì´ë©´ truncate_to_ten í•¨ìˆ˜ í˜¸ì¶œ
            else:
                truncated_data = truncate_to_ten(df[col])
                df[col] = truncated_data  # ë³€í™˜ëœ ì •ìˆ˜í˜• ë°ì´í„°ë¡œ ëŒ€ì²´
        else:
            print(f'Method Not Available for column {col}')

        # ì‹¤ìˆ˜í˜•ì´ë©´ truncate_to_integer í•¨ìˆ˜ í˜¸ì¶œ
        if np.issubdtype(df[col].dtype, np.floating):  # ì‹¤ìˆ˜í˜• í™•ì¸
            truncated_data = truncate_to_integer(df[col])
            df[col] = truncated_data  # ë³€í™˜ëœ ì •ìˆ˜í˜• ë°ì´í„°ë¡œ ëŒ€ì²´

    return df

#ì•ˆì”€ - make_imputer_pipe ë¡œ ëŒ€ì²´
def ohe(df):
    df = df.copy()   
    cols = config_dict['ohe']
    for col in cols:
        trans = OneHotEncoder()
        X = df[[col]]
        trans.fit(X)
        df[col] = trans.transform(X)[col]
    return df

#ì´ìƒì¹˜ íƒìƒ‰ì„ ìœ„í•œ í‰ê· ê°’ êµ¬í•˜ê¸°
def find_boundaries(df, variable, distance):
    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)
    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)
    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)
    return upper_boundary, lower_boundary

#ì´ìƒì¹˜ ì œê±° í•¨ìˆ˜
def outlier(df):
    df = df.copy()
    cols = config_dict['outlier']
    for c in cols:
        upper_limit, lower_limit = find_boundaries(df, c, config_dict['iqr'])
        outliers_ = np.where(df[c] > upper_limit, True,
                    np.where(df[c] < lower_limit, True, False))
        df = df.loc[~(outliers_)]
    return df    

#json ë°ì´í„° ì²˜ë¦¬
def extract_json_data(df):
    """
    ì£¼ì–´ì§„ JSON ë¬¸ìì—´ ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

    Parameters:
    json_column_data (list): JSON ë¬¸ìì—´ì´ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸

    Returns:
    DataFrame: JSON ë°ì´í„°ê°€ í¬í•¨ëœ ìƒˆë¡œìš´ DataFrame
    """
    cols = config_dict['dict_col']
    df = df.copy()
    for col in cols:
        json_column_data = df[col].tolist()
        # JSON ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        json_records = []

        for json_str in json_column_data:
            try:
                # JSON ë¬¸ìì—´ì„ íŒŒì‹±
                json_data = json.loads(json_str)
                json_records.append(json_data)
            except (json.JSONDecodeError, TypeError) as e:
                print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                json_records.append({})  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì¶”ê°€
        
        # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        json_df = pd.DataFrame(json_records)

        # ìƒˆë¡œìš´ ì»¬ëŸ¼ëª… ìƒì„±: ê¸°ì¡´ ì»¬ëŸ¼ëª… + "_" + JSON í‚¤
        new_column_names = {key: f"{col}_{key}" for key in json_df.columns}
        
        # ìƒˆë¡œìš´ ì»¬ëŸ¼ëª…ìœ¼ë¡œ DataFrameì˜ ì»¬ëŸ¼ëª… ë³€ê²½
        json_df.rename(columns=new_column_names, inplace=True)
        insert_at = df.columns.get_loc(col)
        df.drop(columns=[col], inplace=True)
        left = df.iloc[:, :insert_at]
        right = df.iloc[:, insert_at:]
        df = pd.concat([left, json_df, right], axis=1)
    return df
# ìƒˆë¡œìš´ í´ë˜ìŠ¤: ë²¡í„°í˜• â†’ ì†Œìˆ˜í˜• (PCA)
class VectorPCAProcessor:
    def __init__(self, variables, n_components=3):
        self.variables = variables
        self.n_components = n_components
        self.pca = PCA(n_components=n_components)
    
    def fit(self, X, y=None):
        for col in self.variables:
            # ë²¡í„°í˜• ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            vectors = [eval(vec) if isinstance(vec, str) else vec for vec in X[col]]
            vectors = np.array(vectors)
            self.pca.fit(vectors)
        return self
    
    def transform(self, X):
        X = X.copy()
        for col in self.variables:
            # ë²¡í„°í˜• ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            vectors = [eval(vec) if isinstance(vec, str) else vec for vec in X[col]]
            vectors = np.array(vectors)
            # PCA ì ìš©
            transformed = self.pca.transform(vectors)
            # ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€
            for i in range(self.n_components):
                X[f'{col}_pca_{i}'] = transformed[:, i]
            # ì›ë³¸ ì»¬ëŸ¼ ì‚­ì œ
            X.drop(columns=[col], inplace=True)
        return X

# ìƒˆë¡œìš´ í•¨ìˆ˜: ì§„ë²•í˜• â†’ ì •ìˆ˜í˜•
def convert_non_decimal(df):
    df = df.copy()
    cols = config_dict.get('non_dec_col', [])
    if not cols or pd.isna(cols):
        return df
    for col in cols:
        def parse_non_decimal(val):
            try:
                if isinstance(val, str):
                    val = val.lower().strip()
                    if val.startswith('0b'):
                        return int(val, 2)  # 2ì§„ìˆ˜
                    elif val.startswith('0x'):
                        return int(val, 16)  # 16ì§„ìˆ˜
                    else:
                        return int(val)  # 10ì§„ìˆ˜ ê°€ì •
                return val
            except (ValueError, TypeError):
                return np.nan
        insert_at = df.columns.get_loc(col)
        new_col = df[col].apply(parse_non_decimal)
        new_col.name = f'dec_{col}'
        df.drop(columns=[col], inplace=True)
        left = df.iloc[:, :insert_at]
        right = df.iloc[:, insert_at:]
        df = pd.concat([left, new_col.to_frame(), right], axis=1)
    cols = position_Y_COL(list(df.columns))
    return df[cols]

# ìƒˆë¡œìš´ í•¨ìˆ˜: ë¬¸ì¥í˜• â†’ ë²¡í„°í˜•
def sentence_to_vector(df):
    df = df.copy()
    cols = config_dict.get('sentence_col', [])
    if not cols or pd.isna(cols):
        return df
    model_name = config_dict.get('embedding_model', 'sentence-transformers/all-MiniLM-L6-v2')
    model = SentenceTransformer(model_name)
    for col in cols:
        insert_at = df.columns.get_loc(col)
        sentences = df[col].fillna('').tolist()
        embeddings = model.encode(sentences, show_progress_bar=False)
        # ë²¡í„°ë¥¼ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
        new_cols = {f"{col}_vec_{i}": embeddings[:, i] for i in range(embeddings.shape[1])}
        new_df = pd.DataFrame(new_cols, index=df.index)  # ì›ë˜ ì¸ë±ìŠ¤ ìœ ì§€

        # ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°
        df.drop(columns=[col], inplace=True)

        # ê¸°ì¡´ ìœ„ì¹˜ì— ì‚½ì…
        left = df.iloc[:, :insert_at]
        right = df.iloc[:, insert_at:]
        df = pd.concat([left, new_df, right], axis=1)
    cols = position_Y_COL(list(df.columns))
    return df[cols]

# # ìƒˆë¡œìš´ í•¨ìˆ˜: ë¬¸ì¥í˜• â†’ ë²¡í„°í˜•
# def sentence_to_vector(df):
#     df = df.copy()
#     cols = config_dict.get('sentence_col', [])
#     if not cols or pd.isna(cols):
#         return df
#     model_name = config_dict.get('embedding_model', 'sentence-transformers/all-MiniLM-L6-v2')
#     model = SentenceTransformer(model_name)
#     for col in cols:
#         sentences = df[col].fillna('').tolist()
#         vector_records = []
#         embeddings = model.encode(sentences, show_progress_bar=False)
#         # ë²¡í„°ë¥¼ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
#         for i in range(embeddings.shape[1]):
#             vector_records.append(embeddings[:, i])
#         vector_df = pd.DataFrame(vector_records)
#         new_column_names = {key: f"{col}_{key}" for key in vector_df.columns}
#         df.drop(columns=[col], inplace=True)
#     cols = position_Y_COL(list(df.columns))
#     return df[cols]


#ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def organize_data(df, y_null_exist):
    df = df.copy()
    cols = list(df.columns)
    cols.remove(Y_COL)
    null_threshhold_cols = []
    discrete, continuous, categorical = discrete_cont(df)
    # - discrete: ì´ì‚°í˜• ë³€ìˆ˜
    # - continuous: ì—°ì†í˜• ë³€ìˆ˜
    # - categorical: ë²”ì£¼í˜• ë³€ìˆ˜

    #ë„ ë¹„ìœ¨ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ì€ ì»¬ëŸ¼ ëª…ì„¸ ì‘ì„±
    for col in cols:
        null_mean = df[col].isnull().mean() # ê° ì»¬ëŸ¼ì˜ null ë¹„ìœ¨ ê³„ì‚°
        if null_mean >= config_dict['null_threshhold']:
            null_threshhold_cols.append(col)

    #ì„ê³„ì¹˜ë¥¼ ë„˜ì€ ëŒ€ìƒì„ ë¹¼ê³  ë‹¤ì‹œ df ë§Œë“¬
    cols_stayed = [c for c in cols if c not in null_threshhold_cols]
    df = df[cols_stayed+[Y_COL]].copy()

    if y_null_exist:
        df = df[df[Y_COL] != df[Y_COL].max()].copy()

    return df, discrete, continuous, categorical

def make_train_test(df):
    df = df.copy()
    X = df.drop(columns=Y_COL)
    y = df[Y_COL]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config_dict['test_size'], random_state=0, stratify=y)
    return X_train, X_test, y_train, y_test

#ê° í˜•íƒœë³„ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì¶”ê°€í•˜ëŠ” í˜•íƒœë¡œ ë³€ê²½
def make_imputer_pipe_old(continuous, discrete, categorical, null_impute_type):
    # ì—°ì†í˜• ë³€ìˆ˜ì™€ ì´ì‚°í˜• ë³€ìˆ˜ë¥¼ í•©ì³ì„œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ì²˜ë¦¬
    numberImputer = continuous + discrete

    categoricalImputer = categorical.copy()
    # One-Hot Encoding ëŒ€ìƒ ë³€ìˆ˜ ì œì™¸
    categoricalImputer = [item for item in categoricalImputer if (item not in config_dict['ohe']) ]
    oheImputer = config_dict['ohe']
    
    result={}
    
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ëª¨ë‘ ìˆëŠ” ê²½ìš°
    if (len(numberImputer) > 0) & (len(categoricalImputer) > 0):
        pipe = Pipeline([
            # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´
            ("imputer",
            mm.MeanMedianImputer2(
                imputation_method=null_impute_type, variables=numberImputer),),
            # ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´
            ('imputer_cat',
            mdi.CategoricalImputer(variables=categorical)),
            # One-Hot Encoding ì ìš©
            ('categorical_encoder',
            ce.OneHotEncoder(variables=oheImputer)),
            # ë¼ë²¨ë§ ì¸ì½”ë”© ì ìš©
            ('categorical_encoder2',
            ce.OrdinalEncoder(encoding_method='ordered',
                variables=categoricalImputer))
        ])
    else:
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ìˆê³  ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°
        if (len(numberImputer) > 0) & (len(categoricalImputer) == 0):
            pipe = Pipeline([
                # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ë§Œ ëŒ€ì²´
                ("imputer",
                mm.MeanMedianImputer2(
                    imputation_method=null_impute_type, variables=numberImputer),)
            ])
        else:
            # ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ìˆê³  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°
            if (len(numberImputer) == 0) & (len(categoricalImputer) > 0):
                pipe = Pipeline([
                    # ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´
                    ('imputer_cat',
                    mdi.CategoricalImputer(variables=categorical)),
                    ('categorical_encoder',
                    ce.OneHotEncoder(variables=oheImputer)),
                    ('categorical_encoder2',
                    ce.OrdinalEncoder(encoding_method='ordered',
                        variables=categoricalImputer))
                ])
            else:
                pipe = []
    return pipe

def make_imputer_pipe(continuous, discrete, categorical, null_impute_type):
    # ì—°ì†í˜• ë³€ìˆ˜ì™€ ì´ì‚°í˜• ë³€ìˆ˜ë¥¼ í•©ì³ì„œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ì²˜ë¦¬
    numberImputer = continuous + discrete

    categoricalImputer = categorical.copy()
    # One-Hot Encoding ëŒ€ìƒ ë³€ìˆ˜ ì œì™¸
    categoricalImputer = [item for item in categoricalImputer if (item not in config_dict['ohe']) ]
    oheImputer = config_dict['ohe']
    datecolImputer =  config_dict['date_col'] if config_dict['date_col'] and not pd.isna(config_dict['date_col'][0]) else []
    vectorImputer = config_dict.get('vector_col', []) if config_dict.get('vector_col', []) and not pd.isna(config_dict.get('vector_col', [])[0]) else []
    # result={}
    
    steps = []
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸(ê²°ì¸¡ì¹˜ë¥¼ null_impute_typeê°’[mean,median,max,min]ì— ë”°ë¼ ì±„ì›€)
    if numberImputer  and len(numberImputer) > 0:
        steps.append(("numeric_imputer", mm.MeanMedianImputer2(imputation_method=null_impute_type, variables=numberImputer)))
        # print('processing numeric')

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸(ê²°ì¸¡ì¹˜ë¥¼ ìµœë¹ˆê°’ìœ¼ë¡œ ì±„ì›€)
    if categorical  and len(categorical) > 0:
        steps.append(('categorical_imputer', mdi.CategoricalImputer(variables=categorical)))
        # print('processing categorical')

    # ì›í•«ì¸ì½”ë”© ì²˜ë¦¬(ë°ì´í„° ì¢…ë¥˜ë§Œí¼ ì»¬ëŸ¼ì„ ë§Œë“¤ì–´ 1,0ìœ¼ë¡œ í‘œí˜„)
    if oheImputer and len(oheImputer) > 0:
        steps.append(('onehot_encoder', ce.OneHotEncoder(variables=oheImputer)))
        # print('processing ohe')

    # ë¼ë²¨ ì¸ì½”ë”© ì²˜ë¦¬(ë°ì´í„° ì¢…ë¥˜ë³„ ê³ ìœ ìˆ˜ì¹˜ë¡œ ë³€ê²½, male->1, female->2)
    if categoricalImputer and len(categoricalImputer) > 0:
        steps.append(('label_encoder', CustomOrdinalEncoder(encoding_method='ordered', variables=categoricalImputer)))
        # print('processing label encoding')


    # ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬(ë‚ ì§œí˜•ì—ì„œ ì—°ì›”ì¼ ì¶”ì¶œ, ì‹œê°„í˜•ì—ì„œ íƒ€ì„ë¸íƒ€ ì¶”ì¶œ)
    if datecolImputer and len(datecolImputer) > 0:
        steps.append(('temporal_feature_engineering', tf.DateFeatureTransformer2(variables=datecolImputer, features=['year', 'month', 'day', 'time_seconds'], drop_original=True)))
        # print('processing datecol imputing')
    
    # ë²¡í„° ë°ì´í„° ì²˜ë¦¬
    if vectorImputer and len(vectorImputer) > 0:
        steps.append(('vector_pca', VectorPCAProcessor(variables=vectorImputer, n_components=config_dict.get('pca_components', 3))))
        # print('processing vector imputing')
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    return Pipeline(steps) if steps else []

def restore_column_order(df_transformed, original_order):
    df_transformed = df_transformed.copy()

    # ì›ë˜ ìˆë˜ ì»¬ëŸ¼ ìˆœì„œë¥¼ ìµœëŒ€í•œ ì•ì— ë°°ì¹˜
    ordered_cols = [col for col in original_order if col in df_transformed.columns]

    # ë‚˜ë¨¸ì§€ ìƒˆë¡œ ìƒì„±ëœ ì»¬ëŸ¼ì€ ë’¤ìª½ì— ì •ë ¬
    remaining_cols = [col for col in df_transformed.columns if col not in ordered_cols]

    return df_transformed[ordered_cols + remaining_cols]


def do_imputation(df, pipe, original_order):
    train=False
    if(train):
        xtrain, xtest, y_train, y_test = make_train_test(df)
        
        # pipe.fit(X_train, y_train)
        # íŒŒì´í”„ë¼ì¸ì„ í›ˆë ¨ ë°ì´í„°ì— ë§ì¶¤
        pipe.fit(xtrain, y_train)
        X_train = pipe.transform(xtrain)
        X_test = pipe.transform(xtest)

        # ì»¬ëŸ¼ ë³µì›
        X_train = restore_column_order(X_train, original_order)
        X_test = restore_column_order(X_test, original_order)

        # í›ˆë ¨ ì„¸íŠ¸ì— íƒ€ê²Ÿ ë³€ìˆ˜ì™€ 'split' ì—´ ì¶”ê°€
        X_train[Y_COL] = y_train        
        X_train['split'] = 'train'
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— íƒ€ê²Ÿ ë³€ìˆ˜ì™€ 'split' ì—´ ì¶”ê°€
        X_test[Y_COL] = y_test
        X_test['split'] = 'test'        
        return pd.concat([X_train, X_test]).reset_index(drop=True)
    else:
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ íŒŒì´í”„ë¼ì¸ì„ ì ìš©
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
        y_full = df[Y_COL]
        
        # íŒŒì´í”„ë¼ì¸ì„ ì „ì²´ ë°ì´í„°ì— ë§ì¶¤
        pipe.fit(df.drop(columns=[Y_COL]),y_full)
        
        # ë³€í™˜ ì ìš©
        X_full = pipe.transform(df.drop(columns=[Y_COL]))

        # ì»¬ëŸ¼ ìˆœì„œ ë³µì› ì‹œë„
        X_full = restore_column_order(X_full, original_order)
        # ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„ì— íƒ€ê²Ÿ ë³€ìˆ˜ ì¶”ê°€
        X_full[Y_COL] = y_full
        X_full['split'] = 'full'
        
        return X_full.reset_index(drop=True)

def scaling(df):    
    df = df.copy()
    if config_dict['scale'] is np.nan:
        config_dict['scale'] = ['minmax']   # default with minmax scaling
    if config_dict['scale'][0] =='minmax':
        scaler = MinMaxScaler()
        scaler.fit(df)
        return scaler.transform(df)
    elif config_dict['scale'][0] =='standard':
        scaler = StandardScaler()
        scaler.fit(df)
        return scaler.transform(df)
    else: 
        scaler = MinMaxScaler()
        scaler.fit(df)
        return scaler.transform(df)        


if __name__ == '__main__':
    # arv ì˜ˆ1: credit 
    # arv ì˜ˆ2: metro 

    try:
        #íŒŒë¼ë¯¸í„°
        #folder = sys.argv[1]    # take input with argv parameter
        folder = "loans"    #í…ŒìŠ¤íŠ¸ìš©

        parent = join_abs_path(os.getcwd(), os.pardir)
        conf_file = f'argumet_{folder}.xlsx'      
        configs = pd.read_excel(join_abs_path(f'{parent}/config', conf_file), header=None).set_index(0)        
        config_cols = configs.index.tolist()
        config_dict = {}
        for c in config_cols:
            config_dict[c] = configs.loc[c].values[0]
            if (type(config_dict[c]) == int) or (type(config_dict[c]) == float):
                pass
            else:
                config_dict[c] = configs.loc[c].values[0].split(',')
        ori_file_name = config_dict['file_name'][0].split('.')[0]
        
        if config_dict['mixed_str'] is np.nan or len(config_dict['mixed_str']) < 1:
            pass
        else:
            config_dict['mixed_str'] = [eval(i) for i in config_dict['mixed_str']]  #ë°°ì—´ì˜ ê° ê°’ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜

        if config_dict['y_col'] is np.nan or len(config_dict['y_col']) != 1:
            print('No Y column exists')
            raise Exception

        if config_dict['discrete_thresh_hold'] is np.nan or config_dict['discrete_thresh_hold'] < 0:
            print('discrete_thresh_hold set to default 10')
            config_dict['discrete_thresh_hold'] = 10

        Y_COL = config_dict['y_col'][0]
        original_file = join_abs_path(f'{parent}/data/{folder}', config_dict['file_name'][0])
        df_initial = read_data(original_file)
        original_order = df_initial.drop(columns=[Y_COL]).columns.tolist()  # ì›ë³¸ ì»¬ëŸ¼ ìˆœì„œ ì €ì¥

        # 1. Label column Encoding
        df_labeld, y_null_exist = y_label_enc(df_initial)

        # 1.1json ì²˜ë¦¬
        df_jsoned = extract_json_data(df_labeld)

        # 1.2 ì§„ë²•í˜• ì²˜ë¦¬
        df_non_dec = convert_non_decimal(df_jsoned)

        # 1.3 ë¬¸ì¥í˜• ì²˜ë¦¬
        df_sentenced = sentence_to_vector(df_non_dec)

        # 2. ë°ì´í„° ì •ë¦¬ ë° ë³€ìˆ˜ ë¶„ë¥˜
        df_organized, discrete, continuous, categorical = organize_data(df_sentenced, y_null_exist)

        # 3. Mixed ì¹¼ëŸ¼ì„ ìˆ«ìí˜•/ë¬¸ìí˜•ìœ¼ë¡œ ë¶„ë¦¬(ë¶„ë¦¬ í›„ df_organized, discrete, continuous, categorical ì¬ë¶„ë¥˜)
        if config_dict['mixed'] is not np.nan:
            df = separate_mixed(df_organized)
            discrete, continuous, categorical = discrete_cont(df)
        else:
            df = df_organized.copy()

        # null_impute_types ì •ì˜
        null_impute_types = config_dict['null_imp']

        if null_impute_types is not np.nan:
            for null_impute_type in null_impute_types:
        # 4. pipeline ì •ì˜
                print("ğŸš§ Before calling make_imputer_pipe")
                pipe = make_imputer_pipe(discrete, continuous, categorical, null_impute_type)
                print("âœ… Created pipeline")
                if pipe == []:
                    print('no pipe applied')
                else:
        # 5. discretization(ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ)
                    if config_dict['discretiser'] is not np.nan:
                        df_piped = discretiser(df, discrete+continuous)
        # 6. imputation thru pipeline
                    print("ğŸš§ Before calling do_imputation")
                    df_piped = do_imputation(df, pipe,original_order)
                    print("âœ… Finished imputation")
                    dest_path = os.path.join(parent, os.path.join('data_preprocessed', f'{folder}'))
                    dest_path = os.path.join(parent, os.path.join(dest_path, f'{dest_path}/imputed'))
                    Path(dest_path).mkdir(parents=True, exist_ok=True)
                    dest_path = os.path.join(parent, os.path.join(dest_path, f'imputed_{ori_file_name}_{null_impute_type}.csv'))
                    df_piped.to_csv(dest_path, index=False)

        # 8. discretization(ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ)
                    if config_dict['discretiser'] is not np.nan:
                        df_piped = discretiser(df_piped, discrete+continuous)
        
        # 9. Outlier ì²˜ë¦¬
                    if config_dict['outlier'] is not np.nan:    
                        df_piped = outlier(df_piped)
                        df_piped = df_piped.reset_index(drop=True)
        # 9.1 ë°ì´í„° ì •ì œ ì €ì¥
                    dest_path = os.path.join(parent, os.path.join('data_preprocessed', f'{folder}'))
                    dest_path = os.path.join(parent, os.path.join(dest_path, 'trans'))
                    Path(dest_path).mkdir(parents=True, exist_ok=True)
                    dest_path = os.path.join(parent, os.path.join(dest_path, f'trans_{ori_file_name}_{null_impute_type}.csv'))
                    df_piped.to_csv(dest_path, index=False)


        # 10. ìŠ¤ì¼€ì¼ë§ ì‘ì—… ë° ì €ì¥/ Trainê³¼ Test ë¥¼ ë”°ë¡œ ìŠ¤ì¼€ì¼ë§
        # 10.1 X_train ìŠ¤ì¼€ì¼ë§
                    con = df_piped['split'] == 'train'
                    if not df_piped[con].empty:
                        X_train_scaled = scaling(df_piped[con].drop(columns=[Y_COL,'split']))
                        X_train_scaled = pd.DataFrame(X_train_scaled)
                        X_train_scaled[Y_COL] = df_piped[con][Y_COL]
                        X_train_scaled['split'] = df_piped[con]['split']
                        X_train_scaled.columns = df_piped.columns
                    else:
                        X_train_scaled = []
        # 10.2 X_test ìŠ¤ì¼€ì¼ë§
                    con = df_piped['split'] == 'test'
                    X_test_scaled = []
                    if not df_piped[con].empty:
                        X_test_scaled = scaling(df_piped[con].drop(columns=[Y_COL,'split']))
                        X_test_scaled = pd.DataFrame(X_test_scaled)
                        tmp = df_piped.copy().reset_index()
                        X_test_scaled['index'] = tmp[con]['index'].values
                        X_test_scaled = X_test_scaled.set_index('index')
                        X_test_scaled[Y_COL] = df_piped[con][Y_COL]
                        X_test_scaled['split'] = df_piped[con]['split']
                        X_test_scaled.columns = df_piped.columns
                        X_test_scaled.index.name = None
                        del tmp
                    else:
                        X_test_scaled = []        
        # 10.3 data frame merge                                        
                    if (len(X_train_scaled) == 0 and len(X_test_scaled) == 0 ):
                        df_scaled = scaling(df_piped.drop(columns=[Y_COL,'split']))
                        df_scaled = pd.DataFrame(df_scaled)
                        df_scaled[Y_COL] = df_piped[Y_COL]
                        df_scaled['split'] = df_piped['split']
                        df_scaled.columns = df_piped.columns
                    else :
                        df_scaled = pd.concat([X_train_scaled, X_test_scaled])
        # 10.4 scaling ì €ì¥                        
                    dest_path = os.path.join(parent, os.path.join('data_preprocessed', f'{folder}'))
                    dest_path = os.path.join(parent, os.path.join(dest_path, 'scaled'))
                    Path(dest_path).mkdir(parents=True, exist_ok=True)
                    dest_path = os.path.join(parent, os.path.join(dest_path, f'scaled_{ori_file_name}_{null_impute_type}.csv'))
                    df_scaled.to_csv(dest_path, index=False)
        print('Completed.')
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        print('ë¹„ì •ìƒì¢…ë£Œ', e)
        traceback.print_exc()
        print(exc_type, exc_tb.tb_lineno)